<!DOCTYPE html>
    <html lang="en">

    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
      <title>Transcription: Podcast 0148 - Douglas Eck</title>

      <!-- Bootstrap -->
      <link href="css/bootstrap.min.css" rel="stylesheet">

      <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
      <!-- WARNING: Respond.js does not work if you view the page via file:// -->
      <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->
      
    </head>

    <body>
      <div class="container">
        <h2>Transcription: 0148 - Douglas Eck</h2>
        <h3>Released: Oct 23, 2016</h3>
    <br>
    <p>
      <iframe style="border: none" src="//html5-player.libsyn.com/embed/episode/id/4766725/height/90/theme/custom/thumbnail/yes/direction/forward/render-playlist/no/custom-color/000000/" height="90" width="100%" scrolling="no"  allowfullscreen webkitallowfullscreen mozallowfullscreen oallowfullscreen msallowfullscreen></iframe>
    </p>
    <br>
    <p>
<b>Darwin: </b>Okay, today I have the enjoyable pleasure of introducing you to someone that I've gotten to know recently. His name is Douglas Eck He, works for Google. He works as part of the Google Brain team, and, he is heading up a project called The Magenta Project. It's really pretty fascinating, but I'm also gonna do it poor service if I try to explain it. So what we'll do is we'll talk to Doug and let him explain it. So with that, I say hi to Doug. Hi, Doug. How are you? 
</p>

<p>
<b>Douglas Eck: </b>I'm great. How are you doing, Darwin? 
</p>

<p>
<b>Darwin: </b>I'm all right. 
</p>

<p>
<b>Douglas: </b>You're all right. It's Friday. 
</p>

<p>
<b>Darwin: </b>Why don't, why don't, right. It is Friday. Why don't we kick this off by having you, describe what your project is all about? 
</p>

<p>
<b>Douglas: </b>Sure. The project is called Magenta, and the, the goal of Magenta is to understand a little better, whether we can use deep learning and reinforcement learning to generate media that people actually like. And by media, I mean music and visual art and video and text. I think the real, the real interesting question that we're getting at, it's not does anybody wanna sit back and push a button and listen to the output of a computer or look at the art generated by a computer, but rather can artificial intelligence and specifically this kind of machine learning, be turned into a really interesting creative tool where what you have is some sort of companion that you can work with to, to, to make new stuff that's trying to be smart with you and engage with you. You know, can we make computers smarter? Can we make them more artistic? So <span style="color:red" data-ts="104.085" data-end_ts="104.245" title="104.085" id=c_"104.085">__that__</span> new tools to work with, that's the, that's the goal. 
</p>

<p>
<b>Darwin: </b>Well, that sounds really interesting, but it also seems a little bit intimidating. What in your, well, first of all, before we get into what that means to me as a musician, let's talk a little bit about some of the terminology used. So, for people who aren't familiar, this is all wrapped up in the concepts behind machine learning, right? Yeah, 
</p>

<p>
<b>Douglas: </b>That's correct. 
</p>

<p>
<b>Darwin: </b>And you've used two phrases. One was deep learning, and the other was reinforcement learning. Could you explain a little bit what that means? 
</p>

<p>
<b>Douglas: </b>Absolutely. Deep learning is, a form of artificial intelligence where we are trying to understand the world a little bit better by looking at data from the world. We have some data, maybe some photographs that are labeled with the objects that are in the photograph. Like imagine in your mind a photograph of a, of a car and, a baby stroller sitting next to the car. And those labels are there in, in deep learning, or I guess any kind of neural network learning. And I'll come back to, how that relates to deep learning. <span style="color:red" data-ts="167.705" data-end_ts="167.925" title="167.705" id=c_"167.705">__In__</span>, in these sorts of models, what we do is we try to warp the data, shift the data, and move it around using neural networks so that we can, predict what's there. We call it deep learning, because these networks tend to work better when they're made, deeper and deeper with multiple layers. 
</p>

<p>
<b>Douglas: </b>I see. Lemme say this in even a, a more, high level fashion. Brains learn by making correlations with the world, and when they see correlations strengths, connections between neurons and the brain are strengthened. Mm-Hmm, <affirmative> and roughly the same. That same trick is being used to train artificial neural networks, which, like the brain have neurons, or they're, they're mathematical neurons. They're not real brain neurons, but they're connected together, and there are weights on those connections. And the, the, the game that we play, so to speak, to train them, is to set those weights so that the model is able to draw some inferences about what's going on in the world. Oh, look, from that pix from the pixels in that image, I, I think there's a car there. And from the pixels in that image, I think that's a baby stroller right next to it. 
</p>

<p>
<b>Darwin: </b>Right. Got it. Okay. And reinforcement learning means what? 
</p>

<p>
<b>Douglas: </b>Reinforcement learning adds some extra sauce to the, to the mix. So if we think about what we're we're doing with the, the deep learning that I described, we have labeled data, we have a picture, and that picture has some labels attached to it. Those labels were given to us by people, right? Hey, there's a human at some point that realized there is a, there's a, an automobile and a baby stroller in the picture. Let's shift to another task. We wanna have a robot that can navigate an office building and, maybe vacuum the floors or be your personal assistant. There's not enough labeled data in the world to train that robot using, the kind of learning I described with the image. Supervised learning, because we have the labels as if to say, Hey, this is a car. And so robots have to learn a little bit more in, in the way, I guess maybe babies learn, which is they explore the world and they get some feedback that, that was good, that was bad. 
</p>

<p>
<b>Douglas: </b>That feedback comes to these, these reinforcement learning algorithms in the same way as reward. And these models learn from, from that reward. So the, the crucial distinction to make between deep learning and reinforcement learning is the information that's coming to the computer program <span style="color:red" data-ts="302.585" data-end_ts="302.805" title="302.585" id=c_"302.585">__in__</span> supervised learning. The answer is being given to the program. Hey, that was a car. Okay. In reinforcement learning, instead, it's a, a reward like, Hey, you did better, or, Hey, you did worse, but I'm not telling you why you did better. I'm just telling you, you did better keep up the good work. 
</p>

<p>
<b>Darwin: </b>Interesting. So it doesn't attempt to, it doesn't attempt to like debug the problem. It just says good or no good or better or not as good. 
</p>

<p>
<b>Douglas: </b>Yeah. So there's a really nice, a really nice, bit of behavior that you get that in fact looks like debugging. And that is the way that you train a reinforcement learning program, is you kind of set it loose on a, a simulated version of the world, and you give it reward and it just gets to try out anything it wants to try out. And it just keeps trying and trying and trying until it figures out some way to get positive reward. You know, if you've raised small children, you may note that they often do the same thing when they want something like, you know, candy or to go do stay up late, they'll try anything to get that reward. I think, you know, in flavor, that's really not a dishonest way to talk about reinforcement learning. And, this actually helps train models and makes them qualitatively very different than the kind of models that you get when you're working only with supervised learning, for example, deep learning. 
</p>

<p>
<b>Darwin: </b>Right, right. That makes a lot of sense. So it's fairly easy for us to imagine this in terms of like visual, visual tooling, and I think we've all seen it in action, whether it's a, a photo program identifying faces for us, or, you know, we're all reading with wrapped attention on what's happening with, with cars that are able to negotiate the world on their own and stuff like that. But how does this, how does this relate to audio and or performance content in a way that's useful for this magenta project? 
</p>

<p>
<b>Douglas: </b>It's a good question. It's also, you know, relevant for your, for, for the kind of work that you do and, and, and your listeners. So how does this work? Let's unpack this for a second. So, at the highest level, like the simplest sort of example of a thing that I would love for us to build in Magenta is a really smart drum machine. <span style="color:red" data-ts="429.425" data-end_ts="429.645" title="429.425" id=c_"429.425">__So__</span> take that as an example. And I think actually the 8 0 8 is already freaking awesome, right? So, you know, we've gotten a lot out of the kind of drum machines that that, that we were given, you know, over the last, what, 20 years. Imagine now a drum machine that can listen, a drum machine that's paying attention to the musical context, and based upon what it's been trained on, maybe it's been trained on millions of hours of audio, it's listened to lots of music, and it has some idea of what's going on in terms of drumming. 
</p>

<p>
<b>Douglas: </b>This is a thought experiment. We're not releasing a drum machine this good in the next, you know, <laugh> the next two months. But it's a goal. So now imagine you're a guitarist, I'm a guitarist, and, I want <span style="color:red" data-ts="466.485" data-end_ts="466.605" title="466.485" id=c_"466.485">__to__</span> have, you know, I don't have a drummer, over tonight. You know, I want to sit around on Friday evening, and I wanna, I wanna have a beer, and I wanna noodle a little bit on the guitar. And so I want to turn on this really smart drummer. Now, there's a couple of things you could hope for from this drummer. The first one is that it listens and it responds appropriately and appropriately amounts to, it responds in a way that, that I like, that I like to play with, right? Right. That's the goal. And so you can imagine in a reinforcement learning setting that we could have some way of saying, Hey, I'm going to start playing with this, this Magento model. 
</p>

<p>
<b>Douglas: </b>It's gonna start drumming along with me, and I'm gonna have some, some way, maybe a pedal, maybe it's just, you know, somehow picking up on my playing, say, oh, yeah, I like this. Let's keep doing this, or, I don't like this, let's not do that. And then crucially, and I say crucially, because this is what the 8 0 8 can't do right now. It actually in real time <span style="color:red" data-ts="515.785" data-end_ts="516.005" title="515.785" id=c_"515.785">__and__</span> performance time responds to the setting and <span style="color:red" data-ts="518.565" data-end_ts="518.765" title="518.565" id=c_"518.565">__makes__</span> some interesting musical decisions, and those decisions would be fun for me to play along with. So that's, that's the kind of thing we, we strive to build. And, it's a good example of something where you would care about both reinforcement learning and, and deep neural networks. And we can unpack that if you'd like. I mean, there are lots of ways to skimm this cat, so to speak. You, you can solve this using other problems. But, but we think, that, that the direction to go in is the, this combination and, and we're betting on it. 
</p>

<p>
<b>Darwin: </b>Sure. Now, one of the things that this sort of implies is the ability for the neural network and for these learning systems to be able to decode the data from existing stuff. So for example, if I wanted something that could do a, do a good Charlie Parker <span style="color:red" data-ts="563.155" data-end_ts="563.445" title="563.155" id=c_"563.155">__like__</span> sax solo, for me, I would need to have it understand a lot of Charlie Parker's sax solos. Right? But how do I get them, how do, are you working on, are you having to deal with the problems of encoding all of this data in some useful machine form, or is that, or is that just still too hard or what, you know, I don't, I don't even know <span style="color:red" data-ts="590.805" data-end_ts="590.885" title="590.805" id=c_"590.805">__what__</span> how to <span style="color:red" data-ts="591.345" data-end_ts="591.565" title="591.345" id=c_"591.345">__ask__</span> that question, right? 
</p>

<p>
<b>Douglas: </b>No, no, it's a, it's a great question. And here's why. Like, in all machine learning, it's, it's sort of a running joke in the amongst machine learning practitioners that machine learning is 80% plumbing and data, right? It's great to have these to talk in, in, you know, big grandiose terms about machine learning, such as deep learning and these huge neural networks. At the end of the day, it's all about getting the data. And yeah, in music, it's hard because we have, I think we have two very useful levels at which to talk about music. As a musician, this should make sense to you. And certainly given your background, you have representations that are fundamentally capturing notes and, and signal surrounding notes. So you have midi and, and, and other sorts of formats like that. And then you just have audio, right? Right. And there's, there are benefits to working with both. 
</p>

<p>
<b>Douglas: </b>Yeah. It turns out that getting a, computer to make, you know, Charlie Parker, like solos would require a lot, at least with what we have now, would require a lot of of work in terms of getting the right data in a sense, to just show these algorithms. What is Charlie Parker in the first place, right? You could give it all the audio from all the Charlie Parker works that we could collect. Even then, we'd somehow have to show the model, Hey, you should pay attention to the saxophone. And well, what's that? You know, how do you identify that in the audio? Right? But let's just say when you get up in the morning and you try to solve a problem like Charlie, Charlie Parker solos, even if you feel like machine learning is the way to go, it makes you humble. And I think one translation of humble is Uhoh, I gotta go get the data, and that's gonna take me a long time. And wow. Humans are really great <laugh>, seriously, like, this is a really hard problem. Well, 
</p>

<p>
<b>Darwin: </b>And, and it is actually kind of interesting because, I mean, it, it does have to be humbling because there is this sense to which you're doing all of this work. You're, you're putting all of this effort into it to try and sort of decode how <span style="color:red" data-ts="702.085" data-end_ts="702.405" title="702.085" id=c_"702.085">__child__</span>, how a child learns when a child learns. So naturally it has to be frustrating to say, wow, you know, within three years a child can walk and start talking and can throw a thing and likely hit the dog, right? <laugh>. 
</p>

<p>
<b>Douglas: </b>Yeah. 
</p>

<p>
<b>Darwin: </b>And, no, I 
</p>

<p>
<b>Douglas: </b>Believe me, it's, it's, I I think in the same way that like, brain surgeons are the most humble about how the real brain works, right? You know, those of us that are looking at artificial intelligence, you know, you, you run up against a very, very, very interesting, set of problems when you start to try to tackle intelligence. Sure. You know, I think one, one thing that makes makes me hopeful for Magenta that makes me feel like we're not just Don Quixote's tilting at windmills, is that we're really trying to build tools that people can use. The analogy I'm thinking about is, Rickenbacker or Gibson inventing the electric guitar, then watching what people like Jimi Hendrix did with the electric guitar. It, it wasn't necessary for Rickenbacker to have the vision, you know, to understand what someone like Jimi Hendrix would do with that guitar. 
</p>

<p>
<b>Douglas: </b>Right. You know, his job was to do the engineering and to build a new piece of new piece of equipment. Right. And I think that I, I don't know, I don't think we can solve intelligence where it's capital s solve and capital I intelligence. I, I mean, I think intelligence is ill-defined and organisms are just stunningly complicated and wonderful. I think we can build some really awesome musical and artistic tools for people, and that we, if we're lucky and things go the right way, can actually be part of changing what it means to produce and consume media. Right. And that's humans <span style="color:red" data-ts="798.065" data-end_ts="798.285" title="798.065" id=c_"798.065">__and__</span> loop all the way down. It's still audacious. It's still a very difficult task, but it's not this task of sitting in a room alone and saying, oh, can I solve ai? Like, that's not the task. The task is, can we push some of this technology built in different ways into the world and watch people play with it and learn from it, and maybe, hopefully have algorithms that themselves will learn from this interaction and get better. So the next time you play with this drummer or saxophone player, it actually knows a little bit more. 'cause it was, it was listening to your criticism, and it was listening to your positive feedback. 
</p>

<p>
<b>Darwin: </b>Right. And I, I actually think that, that some of that stuff is, is really important to consider because it's different than what we've seen in, in like, the world of generative systems. Right? 'cause oftentimes, I, I'm trying to remember, I'm struggling to remember the name of this software package that used to, it used to have little players that you could add, right? And if you added, added the guitar player, he'd kind of like strum along while you did your thing. Mm-Hmm. <affirmative>. And, you know, it was clear that first of all, it had a very limited palette. Secondly, it was never gonna do anything more than the very thing it was programmed to do. Right. And it seemed like, seems like one of the things that machine learning offers the opportunity to do was something that I used to think about with this stuff, which is like, you know, maybe if I could take that virtual guitar player and have him have a virtual tab of acid, maybe he would play in a more interesting way. Right? You know, the ability to sort of like generate a, like systemic or cellular, cellular change to the player to be more suitable for what I want to accomplish seems like something that you are actually looking forward to doing. 
</p>

<p>
<b>Douglas: </b>First, I, I love vir the idea of virtual acid. You know, we, we, we talk about sex, drugs, and rock and roll being the fundamental components of <laugh> of the musical experience in the last 40 years, right? <laugh>. Mm-Hmm. <affirmative>. Let's, let's <laugh>, let's just go ahead and make it all, you know, based upon machine 
</p>

<p>
<b>Darwin: </b>Learning. Right. Perfect. 
</p>

<p>
<b>Douglas: </b>I suspect that no matter what we do, what I'd like to do is raise the bar. So you probably know that when the first, phonograph recordings came out that the Edisons of the time would go on stage and they would play the cylinder, and the people in the room would say, oh, it sounds exactly like a symphony. It's just, if the people are there, I can't tell the difference. Right. 'cause they just weren't used to that technology. And then, you know, as we move forward, like, I'm hoping that we can build some things that are like, that seem amazing and seem magical with the full expectation that after people play with 'em for five years, they're gonna be like, oh yeah, that magenta stuff from, you know, 2017. It seemed magical at the time, but wow, look what we have now. Like, you know, just trying to keep up and, you know, the bar is always moving. 
</p>

<p>
<b>Darwin: </b>Yeah, that's, that's actually a great point. The idea that our expectations always push beyond what we currently have is probably the thing that keeps us all employed for a long time too. Right? 
</p>

<p>
<b>Douglas: </b>That's right. <laugh>, you know, there's another, another point I would make that's, that's, I think maybe one of the most important ones is the, the software you described. I think when you said that they sort of drop in and strum along, and maybe I put words in your mouth, but, you know, they're not really adapting. This main goal that I tried to lay out at the beginning of our interview was more about getting machine learning models to listen to and respond to real <span style="color:red" data-ts="990.485" data-end_ts="990.685" title="990.485" id=c_"990.485">__time__</span> feedback from the world to actually be embedded in the world and be listening and be reacting and changing their behavior. That's a goal that transcends music and transcends art. I mean, it's the big goal in, in reinforcement learning and in robotics in general. And in one sense, magenta is just following this trend in the last two or three years in artificial intelligence to really care about that problem. 
</p>

<p>
<b>Douglas: </b>You know, I've chosen to focus on media. 'cause with a, with a mobile phone, you can deliver media, video and images and audio. We have a whole musical community that wants to work with these tools and, also a visual arts community. But at the end of the day, some ways it's trying to bring computing into our lives in a way such that, you know, we have the right <span style="color:red" data-ts="1031.395" data-end_ts="1031.685" title="1031.395" id=c_"1031.395">__kind__</span> of algorithms to, I want, I want the machines to listen to us, right? I, I want us to be driving and I want them to be learning from us and then getting better. And I think, you know, that goal is a, is is one that's, that's shared across the machine learning world. I just wanted to stress it. 
</p>

<p>
<b>Darwin: </b>Sure. Now, what has been, you guys have gone into a couple of shows and kind of shown off some of the early, early efforts that you're, you've put together. What has been the response thus far? 
</p>

<p>
<b>Douglas: </b>We've, we've, released a bunch of audio and rather, sort of midi files that were rendered. There was a, some people from Vietnam, a guy from Vietnam, performed some of this music and then jumped in on YouTube and had a guitarist playing fun. There was a, an interview done in, in Toronto, musicians ringing on top of a, a computer generated melody. And, and we've done, you know, quite a bit more since then, I would say largely, you know, we're, we're just taking baby steps. And most of the, most of the secret sauce that came from these collaborations were, were from the, the, the people that were playing along and adding expressive timing and adding performance dynamics and riffing on top. You know, we just generated a few fun melodies, but, you know, we're just at, you know, we're just getting started. 
</p>

<p>
<b>Douglas: </b>I think overall the response has been surprisingly positive. One of the artists said, I thought I was going to hate playing with this because I know it was made by a computer. Yet it was really fun. And so I think there's a certain fascination, and I think once, once you hear it, and once you sort of realize how limited things are right now, but maybe better than they were before, there's a certain sense of saying, oh, okay, this is okay. This is gonna be fun. Like, I'm in control and I'm getting to drive this, this process with my, with my ideas. Right? And as soon as artists see that and musicians see that the response has been really positive. 
</p>

<p>
<b>Darwin: </b>Yeah. It's that, that seems, you know, it, it actually seems like it being at a <span style="color:red" data-ts="1140.485" data-end_ts="1140.805" title="1140.485" id=c_"1140.485">__really__</span> relatively nascent point at the moment is probably valuable because it's gonna be at the point initially where musicians <span style="color:red" data-ts="1151.065" data-end_ts="1151.285" title="1151.065" id=c_"1151.065">__all__</span> toy with it and say, well, that's quaint and it's fun, and it's a cool little thing to play with as opposed to being, you know, I just, I remember when synthesizers first hit the scene and the musicians unions freaked out because, oh my God, no string player's ever gonna get a chance to play a gig again because of the evil synthesizer. Right? Yeah. It would be nice to be able to take on, you know, some new tools and some new concepts without having to go through that kind of angst again. 
</p>

<p>
<b>Douglas: </b>I agree completely. And I, I have to admit, I, I'm pleasantly surprised so far at, at the response we've gotten that we haven't had that, that angst. Maybe it's because we're not good enough yet, and if we get better, we'll, we'll generate a little, maybe my goal should be to generate a little bit of angst. So I don't think so. I will say, you know, we, I like what you said that, you know, people aren't used to this yet, so there's a certain sense of being able to explore it in a related fashion. I, I had the chance to try out, you know, really good state-of-the-art, virtual reality, you know, put on one of these VR headsets, right? And it's, it's not something that I'm particularly into, but I played around and this was, you know, like, what's really state <span style="color:red" data-ts="1223.485" data-end_ts="1223.605" title="1223.485" id=c_"1223.485">__of__</span> <span style="color:red" data-ts="1223.605" data-end_ts="1223.725" title="1223.605" id=c_"1223.605">__the__</span> art? Like two months ago I was in a research lab here at Google, and frankly, I was blown away. 
</p>

<p>
<b>Douglas: </b>I think the reason I was blown away is it became clear to me that in this virtual reality, system, I, I was utterly willing to suspend disbelief in ways that I'm not willing to. And I would relate it right back to what it must have felt like to be in one of those, theaters where they were showing off the wax cylinder, you know, the phonograph or whatever. And just hearing for the first time this reproduced recording and going, oh my God, it sounds just like the real thing. And I just can't help but think what an amazing direction to go in for, for generative music and generative art than to do it virtual reality. Not because virtual reality is technically better than the world, but because in a virtual reality world, you have people who's com who've completely suspended their disbelief. You can get away with things that you can't get away with otherwise. 
</p>

<p>
<b>Douglas: </b>It's probably why, you know, a lot of audio artists work with, you know, speaker arrays and, and other, you know, bits of technology that kind of throw us out of our normal acoustic domain. And so, yeah, I mean, I think that that gives us avenues in which, especially particularly creative people that know this technology, I hope can run with, with some of the stuff we're doing in Magenta, and like, not just create Charlie Parker solos that fool us into thinking it's Charlie Parker, but just create something completely new, like completely new that I can't imagine. And, you know, <span style="color:red" data-ts="1299.965" data-end_ts="1300.205" title="1299.965" id=c_"1299.965">__none__</span>, neither of us in this interview are imagining right now, but who knows, right? Maybe something cool will come out of this that's actually, gonna generate a new genre. 
</p>

<p>
<b>Darwin: </b>Well, and I think there, there is the opportunity because if, if we take a machine learning system and say, okay, consume this Charlie Parker now consume this tangerine dream, and oh, by the way, my grandpa played this polka, right, <laugh> now, right. Let's see what we have. Right. I think that, I think that that kind of thing is gonna be fairly interesting. 
</p>

<p>
<b>Douglas: </b>There's another part of the puzzle here, which I find, you know, particularly relevant for music, which is this idea of trying to predict what someone else is going to do. Imagine that we treat art creation as a game, and the game is I'm going to create something, some message, and I'm gonna hand you that message. And my hope is that that message will make you happy and, maybe surprise you a little bit and that you're gonna want another message from me. I'm, I'm speaking about it as in terms of passing messages from between people. Because in information theory, that's how they talk about, you know, encoding information. Right? And I think, I think you can think of art as the artist is encoding some information, telling a little story. That story might be musical, it might be visual, it might be, you know, a story. 
</p>

<p>
<b>Douglas: </b>And they're not trying to tell the most obvious story. They're, they're actually trying to make the story interesting to decode because the fun of art is actually figuring it out. And we have all of this related neuroscience evidence that in fact, the brain is rewarded for solving puzzles. The brain is rewarded for learning. And so it's a really interesting story to tell around art. It's that what we're doing is we're creating messages for each other that we can decode. And when we decode them, we actually get pleasure. It's actually fun for us to decode them. 'cause we learn a little something. If you look at the artistic process, that way you can start to think about machine learning models that are trying to have a reasonably good idea of what people are going to be able to understand and not understand, and trying to craft art in such a way that it's super fun for the viewers or the listeners to make sense of. I think if we can understand a little bit of this, we'll start to understand a little bit more about surprise and expectancy and all of these other ideas that we have surrounding what makes art interesting for people. And, of the things that excites me the most, I think that's the thing that excites me the most to be able to try to understand a little bit more about the information, the information content story around art. 
</p>

<p>
<b>Darwin: </b>Right. Well, that also makes, it makes it really clear as to why you're, sort of like anxiously trying to find ways to work directly with artists too. Because finding that mechanism to be that storyteller, whatever that storytelling process might be <span style="color:red" data-ts="1464.465" data-end_ts="1464.685" title="1464.465" id=c_"1464.465">__and__</span> ends up being, probably a key factor in terms of whether you're gonna have valuable result. 
</p>

<p>
<b>Douglas: </b>Thoroughly agree. I completely agree. 
</p>

<p>
<b>Darwin: </b>So I'm curious now, one of the things that we didn't dwell on up to this point is the fact that this is a Google project. You have the, you have the opportunity to work, at the, Google Plex. I'm not sure what you actually call it, but it's an amazing place. I'm wondering to what extent that isn't actually the perfect place to be for things like machine learning and, and learning science. Because in terms of a home of, for massive amounts of data, you've got that. And in terms of working around people that know how to do the plumbing necessary for, for tearing apart data, you got those people too. What are the sort of affordances that you get because of this being part of the Google, the Google Brain team? 
</p>

<p>
<b>Douglas: </b>I think there are, there are positives and negatives to doing this work at Google, to be completely honest, to give, your listeners context, I was, a professor at University of Montreal before coming to Google. I was on sabbatical after getting tenure and had the chance to come here and be a research scientist and jumped at it. I would point out that, it was, you know, February and so quite cold <laugh>. But you know, it's not the reason. And I, every, every, every month, every year I spent as a professor in Montreal, I genuinely loved. And I've loved every month and every year here as a research scientist. So I can imagine this research also happening in academia. I could of course imagine it happening in industry, outside of Google as well. With that context in mind, I think that, that the positives are companies like Google, and I would not say uniquely Google. 
</p>

<p>
<b>Douglas: </b>I would have to throw, you know, other big companies, you know, like, like Facebook and Microsoft. In there as well. We have the engineering skills to work with big data in a way that's, that's not impossible to find in academia, but difficult. And so, you know, our ability to understand how to organize huge data sets and do training, large scale machine learning is, is unparalleled in the world. I'm also happy to say, and, and, and this is, this is surprising, a bit surprising to me still. And I've been here, almost seven years, Google and, and again, other companies. I don't wanna make this an ad for Google, but I'm very proud of Google. They've done a phenomenal job. We've done a good job of genuinely valuing research, especially research in areas like machine learning. And I believe there's a, <span style="color:red" data-ts="1616.925" data-end_ts="1617.045" title="1616.925" id=c_"1616.925">__a__</span>, a genuinely enlightened view that if you can bring great researchers into the company and let them do their work, that in the end the products will get better and the user experiences will get better as well. 
</p>

<p>
<b>Douglas: </b>And I think the evidence for that <span style="color:red" data-ts="1628.825" data-end_ts="1629.045" title="1628.825" id=c_"1628.825">__is__</span>, is there for us in terms of the quality of our speech recognition, our language translation, our ability to do navigation and maps and, and our work, for example, on self-driving cars. So, strangely enough, when I, when I pitched this project to leadership in machine learning and, and research, there was widespread support. And I think the support was just transparent. Yes, people consume media. Yes, people do creative things. We work to live, we don't live to work. And, you know, if we can improve and provide more knowledge, more research and, and better tools for, for these aspects of our lives, that's great and our users are gonna love it. And so, you know, the support for this has been fantastic. I think on the flip side, this could happen in academia as well. I could have written a, a DARPA grant or a National Science Foundation grant and done this as a researcher in an American university. 
</p>

<p>
<b>Douglas: </b>That would've been amazing too. I'm very happy to say that. It's possible to do this kind of relatively blue sky research, at a place like Google without feeling like there's a constant need to turn it into, a product tomorrow or, you know, to completely direct where things are going. And I'm not alone. There are hundred literally hundreds of, of researchers at Google who are just getting to ask questions about how the world works, how machine learning works, how systems work, and, and try to solve them with the ability to publish and to, to, to collaborate with academia. And, and I mean, <laugh> I think it's amazing. I think it's really great. That 
</p>

<p>
<b>Darwin: </b>Is amazing. Well, Douglas, I know that you're busy. I know that I'm kind of impinging on your schedule, so I'm gonna let you go. But I want to thank you for taking time outta your schedule to do this, to chat with us and kind of walk us through, some of what you're up to there. And, again, if people wanna check out this project, where should they look? 
</p>

<p>
<b>Douglas: </b>Yeah, they should look at, you could just search for Google Magenta. There, there is a search engine that will find the way for you. <laugh> <laugh>. Or you could just go <span style="color:red" data-ts="1739.885" data-end_ts="1740.045" title="1739.885" id=c_"1739.885">__to__</span>, you could, you could go directly to magenta.tensorflow.org. That's magenta.tensorflow.org. Magenta's actually a, open source for repository, part of the, the TensorFlow project, which is, sort of our parent project. It's a, a platform for doing machine learning, developed here at Google. Awesome. And I'd want to point point out that I think your stuff is fantastic. I love it and I look forward to, having another lunch here at Google with you. Next time you're in town or maybe elsewhere. Maybe I'll come have lunch in your neck of the woods. 
</p>

<p>
<b>Darwin: </b>That sounds great. Well, thank you so much <span style="color:red" data-ts="1773.215" data-end_ts="1773.565" title="1773.215" id=c_"1773.215">__again__</span>. I'll talk to you again soon. 
</p>

<p>
<b>Douglas: </b>Sounds good. You too. Cheers. 
</p>

<p>
<b>Nobody: </b><i>Alright, </i>
</p>

<p>
<b>Darwin: </b>Many thanks again to Douglas for spending the time and also for like opening the door. I got a chance to visit him at Google a couple of weeks ago. It was really amazing to see the work that they're doing and the team that they put together for this project. If you haven't checked out what TensorFlow is about, or if you've never even heard of the Magento project, you need to check it out because <span style="color:red" data-ts="1804.805" data-end_ts="1804.925" title="1804.805" id=c_"1804.805">__I__</span> think it's an important part of where the future is going for us. I wanted to thank everybody for being continued listeners, for sticking with me throughout the years here. It's, going on your three, four, something like that. It's pretty amazing, that it's been going this long. I started in October some number of years ago and it keeps cranking out. So I wanna thank you for sticking with me and for continuing to listen. Please share this with other people. Send me your comments. I've gotten recently some comments about how to maybe make the audio better, so I'm gonna be working on that. But just in general touch base. Thanks for listening. Catch you soon. Bye.
</p>

    <p><i>Copyright 2016-2024 by Darwin Grosse. All right reserved.</i></p></div>
    <!-- jQuery (necessary for the Bootstrap JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
    
    </body>
    </html>
    